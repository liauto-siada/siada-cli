"""
BugFixAgent.run_enhanced_checker method tests

Tests the enhanced fix result checker functionality, including execution trace analysis and expert-level evaluation
"""
import unittest
from unittest.mock import AsyncMock, MagicMock, patch
import json
from datetime import datetime
from typing import Dict, Any

from siada.agent_hub.coder.bug_fix_agent import BugFixAgent
from siada.foundation.code_agent_context import CodeAgentContext
from siada.services.execution_trace_collector import ExecutionTrace, ModelCall, ToolCall
from agents import RunResult


class TestBugFixAgentRunEnhancedChecker(unittest.IsolatedAsyncioTestCase):
    """Test BugFixAgent.run_enhanced_checker method"""

    def setUp(self):
        """Set up test environment"""
        self.agent = BugFixAgent()
        self.context = CodeAgentContext(root_dir="/test/project")
        self.user_input = "Fix array bounds access issue"
        
        # Create mock RunResult
        self.mock_run_result = MagicMock(spec=RunResult)
        self.mock_run_result.new_items = self._create_mock_new_items()
        self.mock_run_result._extract_execution_trace_from_run_result = MagicMock()

    def _create_mock_new_items(self):
        """Create mock new_items list containing message_output_item and tool_call_output_item"""
        # Create message_output_item
        message_item = MagicMock()
        message_item.type = "message_output_item"
        message_item.raw_item = MagicMock()
        message_item.raw_item.content = "I analyzed the code issue and found need to add boundary check"
        
        # Create tool_call_item (tool call)
        tool_call_item = MagicMock()
        tool_call_item.type = "tool_call_item"
        tool_call_item.arguments = '{"file_path": "src/array_utils.py", "content": "Add boundary check code"}'
        
        # Create tool_call_output_item (tool call result)
        tool_call_output_item = MagicMock()
        tool_call_output_item.type = "tool_call_output_item"
        tool_call_output_item.raw_item = {
            "output": "File successfully modified, added array boundary check",
            "success": True
        }
        
        # Return list containing message_output_item, tool_call_item, tool_call_output_item
        return [message_item, tool_call_item, tool_call_output_item]

    def _create_mock_execution_trace(self) -> ExecutionTrace:
        """Create mock execution trace"""
        # Create mock model call
        model_call = ModelCall(
            call_id=1,
            model="claude-3-sonnet",
            input_messages=[{"role": "user", "content": "Analyze code issue"}],
            output_messages=[{"role": "assistant", "content": "Found array bounds issue"}]
        )
        
        # Create mock tool call
        tool_call = ToolCall(
            call_id=1,
            tool_name="",
            input_args=MagicMock(),
            output_result="File content read",
            duration_ms=150.0,
            error_message=None
        )
        
        # Create execution trace
        trace = ExecutionTrace(
            trace_id="test_trace_001",
            workflow_name="bug_fix",
            start_time= datetime.now(),
            model_calls=[model_call],
            tool_calls=[tool_call],
            total_tokens=1500,
            total_input_tokens=800,
            total_output_tokens=700
        )
        
        return trace

    def _create_mock_enhanced_check_result(self) -> Dict[str, Any]:
        """Create mock enhanced check result"""
        return {
            "is_fixed": True,
            "check_summary": "ğŸ” **ä¸“å®¶åˆ†æ**: æ•°ç»„è¾¹ç•Œæ£€æŸ¥å·²æ­£ç¡®å®ç°ã€‚ğŸ§  **è®¤çŸ¥æ¨¡å¼**: ç³»ç»Ÿæ€§é—®é¢˜åˆ†ææ–¹æ³•ã€‚âš ï¸ **å…³é”®å·®è·**: æ— é‡å¤§é—æ¼ã€‚ğŸ¯ **å½±å“è¯„ä¼°**: ä¿®å¤å®Œæ•´ä¸”å®‰å…¨ã€‚ğŸ’¡ **ç­–ç•¥å»ºè®®**: å»ºè®®æ·»åŠ å•å…ƒæµ‹è¯•éªŒè¯è¾¹ç•Œæ¡ä»¶ã€‚",
            "fix_analysis": "æ¶æ„å½±å“åˆ†æï¼šä¿®å¤ä¸å½±å“æ•´ä½“ç³»ç»Ÿæ¶æ„",
            "trace_analysis": "æ‰§è¡Œç­–ç•¥æœ‰æ•ˆæ€§ï¼šé—®é¢˜è§£å†³æ–¹æ³•ç³»ç»Ÿä¸”å…¨é¢",
            "efficiency_suggestions": [
                "**å…³é”®**: æ·»åŠ è¾¹ç•Œæ¡ä»¶çš„å•å…ƒæµ‹è¯•",
                "**é‡è¦**: è€ƒè™‘ä½¿ç”¨æ›´å®‰å…¨çš„æ•°ç»„è®¿é—®æ¨¡å¼",
                "**å»ºè®®**: æ·»åŠ ä»£ç æ³¨é‡Šè¯´æ˜è¾¹ç•Œæ£€æŸ¥é€»è¾‘"
            ],
            "strategy_suggestions": [
                "**æ¶æ„**: è€ƒè™‘å¼•å…¥ç»Ÿä¸€çš„æ•°ç»„è®¿é—®å·¥å…·ç±»",
                "**æµç¨‹**: å»ºç«‹ä»£ç å®¡æŸ¥æ£€æŸ¥æ¸…å•",
                "**å·¥å…·**: ä½¿ç”¨é™æ€åˆ†æå·¥å…·æ£€æµ‹ç±»ä¼¼é—®é¢˜"
            ],
            "overall_score": 8.5,
            "expert_assessment": {
                "confidence_level": "High",
                "technical_depth_analysis": {
                    "architecture_impact": "ä¿®å¤ä¸å½±å“æ•´ä½“ç³»ç»Ÿæ¶æ„ï¼Œå±€éƒ¨æ”¹è¿›",
                    "integration_concerns": "æ— é›†æˆé£é™©ï¼Œå‘åå…¼å®¹",
                    "performance_implications": "æ€§èƒ½å½±å“å¾®ä¹å…¶å¾®ï¼Œè¾¹ç•Œæ£€æŸ¥å¼€é”€å¾ˆå°",
                    "security_considerations": "æ˜¾è‘—æå‡å®‰å…¨æ€§ï¼Œé˜²æ­¢ç¼“å†²åŒºæº¢å‡º",
                    "maintainability_assessment": "ä»£ç å¯ç»´æŠ¤æ€§è‰¯å¥½ï¼Œé€»è¾‘æ¸…æ™°"
                },
                "cognitive_analysis": {
                    "problem_framing_quality": "é—®é¢˜è¯†åˆ«å‡†ç¡®ï¼Œæ ¹å› åˆ†æåˆ°ä½",
                    "solution_strategy_assessment": "è§£å†³æ–¹æ¡ˆç­–ç•¥åˆç†ï¼Œå®æ–½å¾—å½“",
                    "decision_making_patterns": "å†³ç­–è¿‡ç¨‹é€»è¾‘æ¸…æ™°ï¼Œè€ƒè™‘å…¨é¢",
                    "blind_spots_identified": "æµ‹è¯•è¦†ç›–å¯ä»¥è¿›ä¸€æ­¥åŠ å¼º"
                }
            },
            "execution_intelligence": {
                "strategy_effectiveness": {
                    "overall_approach": "ç³»ç»Ÿæ€§é—®é¢˜è§£å†³æ–¹æ³•ï¼Œæ•ˆæœè‰¯å¥½",
                    "information_gathering": "ä¿¡æ¯æ”¶é›†å……åˆ†ï¼Œä¸Šä¸‹æ–‡ç†è§£å‡†ç¡®",
                    "solution_development": "è§£å†³æ–¹æ¡ˆå¼€å‘è¿‡ç¨‹åˆç†",
                    "validation_strategy": "éªŒè¯ç­–ç•¥åŸºæœ¬å®Œå–„"
                },
                "efficiency_analysis": {
                    "resource_utilization": "è®¡ç®—èµ„æºä½¿ç”¨åˆç†",
                    "workflow_optimization": "å·¥ä½œæµç¨‹åŸºæœ¬ä¼˜åŒ–",
                    "bottleneck_identification": "æœªå‘ç°æ˜æ˜¾æ€§èƒ½ç“¶é¢ˆ",
                    "improvement_opportunities": "å¯ä¼˜åŒ–æµ‹è¯•éªŒè¯ç¯èŠ‚"
                },
                "learning_patterns": {
                    "adaptation_quality": "å¯¹æ–°ä¿¡æ¯é€‚åº”è‰¯å¥½",
                    "insight_generation": "ç”Ÿæˆäº†æœ‰ä»·å€¼çš„æ´å¯Ÿ",
                    "error_recovery": "é”™è¯¯æ¢å¤æœºåˆ¶æœ‰æ•ˆ",
                    "knowledge_integration": "çŸ¥è¯†æ•´åˆèƒ½åŠ›å¼º"
                }
            },
            "professional_recommendations": {
                "immediate_actions": [
                    "**å…³é”®**: æ·»åŠ è¾¹ç•Œæ¡ä»¶çš„å•å…ƒæµ‹è¯•",
                    "**é‡è¦**: éªŒè¯æ‰€æœ‰æ•°ç»„è®¿é—®ç‚¹",
                    "**å»ºè®®**: æ›´æ–°ç›¸å…³æ–‡æ¡£"
                ],
                "strategic_improvements": [
                    "**æ¶æ„**: å»ºç«‹ç»Ÿä¸€çš„å®‰å…¨ç¼–ç æ ‡å‡†",
                    "**æµç¨‹**: å®Œå–„ä»£ç å®¡æŸ¥æµç¨‹",
                    "**å·¥å…·**: é›†æˆé™æ€åˆ†æå·¥å…·"
                ],
                "learning_opportunities": [
                    "**æ¨¡å¼è¯†åˆ«**: å­¦ä¹ è¯†åˆ«ç±»ä¼¼çš„è¾¹ç•Œæ£€æŸ¥é—®é¢˜",
                    "**æŠ€èƒ½å‘å±•**: æå‡å®‰å…¨ç¼–ç æŠ€èƒ½",
                    "**çŸ¥è¯†å·®è·**: æ·±å…¥ç†è§£å†…å­˜å®‰å…¨æœ€ä½³å®è·µ"
                ]
            },
            "risk_assessment": {
                "production_risks": [
                    {
                        "risk_type": "Security",
                        "severity": "Low",
                        "probability": "Low",
                        "description": "ä¿®å¤åå®‰å…¨é£é™©æ˜¾è‘—é™ä½",
                        "mitigation": "ç»§ç»­ç›‘æ§å’Œæµ‹è¯•"
                    }
                ],
                "technical_debt_impact": "æŠ€æœ¯å€ºåŠ¡ç•¥æœ‰å‡å°‘ï¼Œä»£ç è´¨é‡æå‡",
                "regression_potential": "å›å½’é£é™©å¾ˆä½ï¼Œä¿®å¤å‘åå…¼å®¹"
            },
            "quality_metrics": {
                "detailed_scores": {
                    "problem_understanding": 9.0,
                    "solution_completeness": 8.5,
                    "implementation_quality": 8.0,
                    "testing_coverage": 7.0,
                    "documentation_quality": 7.5,
                    "execution_efficiency": 8.5,
                    "strategic_thinking": 8.0
                },
                "score_justification": "æ•´ä½“è´¨é‡è‰¯å¥½ï¼Œä¸»è¦æ”¹è¿›ç©ºé—´åœ¨æµ‹è¯•è¦†ç›–å’Œæ–‡æ¡£å®Œå–„"
            },
            "executive_summary": {
                "verdict": "**ä¸“ä¸šåˆ¤æ–­**: ä¿®å¤è´¨é‡é«˜ï¼Œæœ‰æ•ˆè§£å†³äº†æ ¸å¿ƒé—®é¢˜",
                "key_concerns": "**ä¸»è¦å…³æ³¨ç‚¹**: æµ‹è¯•è¦†ç›–éœ€è¦åŠ å¼ºï¼Œæ–‡æ¡£éœ€è¦æ›´æ–°",
                "success_criteria": "**æˆåŠŸæ ‡å‡†**: è¾¹ç•Œæ£€æŸ¥æ­£ç¡®å®ç°ï¼Œæ— å®‰å…¨æ¼æ´ï¼Œæ€§èƒ½å½±å“å¯æ¥å—",
                "next_steps": "**å»ºè®®åç»­æ­¥éª¤**: 1. æ·»åŠ å•å…ƒæµ‹è¯• 2. æ›´æ–°æ–‡æ¡£ 3. ä»£ç å®¡æŸ¥"
            },
            "code_diff": "diff --git a/array_utils.py b/array_utils.py\n+if index >= 0 and index < len(array):"
        }

    async def test_run_enhanced_checker_with_run_result(self):
        """æµ‹è¯•å¸¦æœ‰ RunResult çš„ run_enhanced_checker æ–¹æ³•"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        mock_diff = "diff --git a/test.py b/test.py\n+if index < len(array):"
        mock_trace = self._create_mock_execution_trace()
        mock_enhanced_result = self._create_mock_enhanced_check_result()
        
        # Mock GitDiffUtil
        with patch('siada.foundation.tools.get_git_diff.GitDiffUtil.get_git_diff_exclude_test_files') as mock_git_diff:
            mock_git_diff.return_value = mock_diff
            
            # Mock _extract_execution_trace_from_run_result æ–¹æ³•
            with patch.object(self.agent, '_extract_execution_trace_from_run_result', return_value=mock_trace):
                
                # Mock enhanced_fix_result_checker.check_with_trace
                self.agent.enhanced_fix_result_checker.check_with_trace = AsyncMock(return_value=mock_enhanced_result)
                
                # æ‰§è¡Œæµ‹è¯•
                result = await self.agent.run_enhanced_checker(
                    self.user_input, 
                    self.context, 
                    self.mock_run_result
                )
            
            # éªŒè¯åŸºæœ¬ç»“æœç»“æ„
            self.assertIsInstance(result, dict)
            self.assertTrue(result["is_fixed"])
            self.assertIn("ä¸“å®¶åˆ†æ", result["check_summary"])
            self.assertEqual(result["overall_score"], 8.5)
            self.assertEqual(result["code_diff"], mock_diff)
            
            # éªŒè¯æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨
            required_fields = [
                "is_fixed", "check_summary", "fix_analysis", "trace_analysis",
                "efficiency_suggestions", "strategy_suggestions", "overall_score",
                "expert_assessment", "execution_intelligence", "professional_recommendations",
                "risk_assessment", "quality_metrics", "executive_summary", "code_diff"
            ]
            
            for field in required_fields:
                self.assertIn(field, result, f"Missing required field: {field}")
            
            # éªŒè¯ä¸“å®¶è¯„ä¼°ç»“æ„
            expert_assessment = result["expert_assessment"]
            self.assertIn("confidence_level", expert_assessment)
            self.assertEqual(expert_assessment["confidence_level"], "High")
            self.assertIn("technical_depth_analysis", expert_assessment)
            self.assertIn("cognitive_analysis", expert_assessment)
            
            # éªŒè¯æŠ€æœ¯æ·±åº¦åˆ†æ
            tech_analysis = expert_assessment["technical_depth_analysis"]
            expected_tech_fields = [
                "architecture_impact", "integration_concerns", "performance_implications",
                "security_considerations", "maintainability_assessment"
            ]
            for field in expected_tech_fields:
                self.assertIn(field, tech_analysis)
            
            # éªŒè¯è®¤çŸ¥åˆ†æ
            cognitive_analysis = expert_assessment["cognitive_analysis"]
            expected_cognitive_fields = [
                "problem_framing_quality", "solution_strategy_assessment",
                "decision_making_patterns", "blind_spots_identified"
            ]
            for field in expected_cognitive_fields:
                self.assertIn(field, cognitive_analysis)
            
            # éªŒè¯æ‰§è¡Œæ™ºèƒ½åˆ†æç»“æ„
            execution_intelligence = result["execution_intelligence"]
            self.assertIn("strategy_effectiveness", execution_intelligence)
            self.assertIn("efficiency_analysis", execution_intelligence)
            self.assertIn("learning_patterns", execution_intelligence)
            
            # éªŒè¯ä¸“ä¸šå»ºè®®ç»“æ„
            professional_recommendations = result["professional_recommendations"]
            self.assertIn("immediate_actions", professional_recommendations)
            self.assertIn("strategic_improvements", professional_recommendations)
            self.assertIn("learning_opportunities", professional_recommendations)
            
            # éªŒè¯å»ºè®®å†…å®¹ä¸ä¸ºç©º
            self.assertTrue(len(professional_recommendations["immediate_actions"]) > 0)
            self.assertTrue(len(professional_recommendations["strategic_improvements"]) > 0)
            self.assertTrue(len(professional_recommendations["learning_opportunities"]) > 0)
            
            # éªŒè¯é£é™©è¯„ä¼°ç»“æ„
            risk_assessment = result["risk_assessment"]
            self.assertIn("production_risks", risk_assessment)
            self.assertIn("technical_debt_impact", risk_assessment)
            self.assertIn("regression_potential", risk_assessment)
            
            # éªŒè¯ç”Ÿäº§é£é™©ç»“æ„
            production_risks = risk_assessment["production_risks"]
            self.assertIsInstance(production_risks, list)
            if production_risks:
                risk = production_risks[0]
                self.assertIn("risk_type", risk)
                self.assertIn("severity", risk)
                self.assertIn("probability", risk)
                self.assertIn("description", risk)
                self.assertIn("mitigation", risk)
            
            # éªŒè¯è´¨é‡æŒ‡æ ‡ç»“æ„
            quality_metrics = result["quality_metrics"]
            self.assertIn("detailed_scores", quality_metrics)
            self.assertIn("score_justification", quality_metrics)
            
            # éªŒè¯è¯¦ç»†è¯„åˆ†
            detailed_scores = quality_metrics["detailed_scores"]
            expected_score_fields = [
                "problem_understanding", "solution_completeness", "implementation_quality",
                "testing_coverage", "documentation_quality", "execution_efficiency", "strategic_thinking"
            ]
            for field in expected_score_fields:
                self.assertIn(field, detailed_scores)
                self.assertIsInstance(detailed_scores[field], (int, float))
                self.assertGreaterEqual(detailed_scores[field], 0)
                self.assertLessEqual(detailed_scores[field], 10)
            
            # éªŒè¯æ‰§è¡Œæ‘˜è¦ç»“æ„
            executive_summary = result["executive_summary"]
            self.assertIn("verdict", executive_summary)
            self.assertIn("key_concerns", executive_summary)
            self.assertIn("success_criteria", executive_summary)
            self.assertIn("next_steps", executive_summary)
            
            # éªŒè¯æ‘˜è¦å†…å®¹ä¸ä¸ºç©º
            for key, value in executive_summary.items():
                self.assertIsInstance(value, str)
                self.assertTrue(len(value.strip()) > 0, f"Empty {key} in executive_summary")
            
            # éªŒè¯æ•ˆç‡å»ºè®®å’Œç­–ç•¥å»ºè®®
            self.assertIsInstance(result["efficiency_suggestions"], list)
            self.assertIsInstance(result["strategy_suggestions"], list)
            self.assertTrue(len(result["efficiency_suggestions"]) > 0)
            self.assertTrue(len(result["strategy_suggestions"]) > 0)
            
            # éªŒè¯ GitDiffUtil è¢«æ­£ç¡®è°ƒç”¨
            mock_git_diff.assert_called_once_with(self.context.root_dir)
            
            # æ³¨æ„ï¼šç”±äºä½¿ç”¨äº† patch.objectï¼Œè¿™é‡Œä¸éœ€è¦éªŒè¯ mock_run_result çš„è°ƒç”¨
            # å®é™…çš„ _extract_execution_trace_from_run_result æ–¹æ³•è¢« patch æ›¿æ¢äº†
            
            # éªŒè¯ enhanced_fix_result_checker.check_with_trace è¢«æ­£ç¡®è°ƒç”¨
            self.agent.enhanced_fix_result_checker.check_with_trace.assert_called_once_with(
                issue_desc=self.user_input,
                fix_code=mock_diff,
                execution_trace=mock_trace
            )
            
            # éªŒè¯è½¨è¿¹æ•°æ®çš„å®Œæ•´æ€§
            self.assertIsNotNone(mock_trace)
            self.assertEqual(mock_trace.trace_id, "test_trace_001")
            self.assertEqual(mock_trace.workflow_name, "bug_fix")
            self.assertEqual(len(mock_trace.model_calls), 1)
            self.assertEqual(len(mock_trace.tool_calls), 1)
            self.assertEqual(mock_trace.total_tokens, 1500)

    def _create_mock_result(self, final_output: str = "é»˜è®¤ä¿®å¤é€»è¾‘") -> MagicMock:
        """åˆ›å»ºæ¨¡æ‹Ÿçš„ RunResult"""
        mock_result = MagicMock(spec=RunResult)
        mock_result.final_output = final_output
        return mock_result

    def _create_mock_check_result(
        self, 
        is_fixed: bool = False, 
        code_diff: str = "é»˜è®¤ä»£ç å·®å¼‚",
        additional_fields: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºæ¨¡æ‹Ÿçš„æ£€æŸ¥ç»“æœ"""
        result = {
            "is_fixed": is_fixed,
            "code_diff": code_diff,
            "analysis": "åŸºç¡€åˆ†æç»“æœ"
        }
        if additional_fields:
            result.update(additional_fields)
        return result


    def test_build_enhanced_feedback_with_execution_stats(self):
        """æµ‹è¯•åŒ…å«æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯çš„åé¦ˆ"""
        current_turn = 0
        result = self._create_mock_result("ä¿®å¤é€»è¾‘")
        check_result = self._create_mock_check_result()
        check_summary = "æ£€æŸ¥æ‘˜è¦"
        enhanced_check_result = self._create_mock_enhanced_check_result()
        enhanced_check_result["execution_stats"] = {
            "model_calls": 5,
            "tool_calls": 8
        }

        feedback = self.agent._build_enhanced_feedback(
            current_turn,
            result,
            check_result,
            check_summary,
            enhanced_check_result
        )

        # éªŒè¯æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯
        self.assertIn("**Execution Statistics:**", feedback)
        self.assertIn("- Model calls: 5", feedback)
        self.assertIn("- Tool calls: 8", feedback)

    async def test_run_enhanced_checker_without_run_result(self):
        """æµ‹è¯•ä¸å¸¦ RunResult çš„ run_enhanced_checker æ–¹æ³•"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        mock_diff = "diff --git a/test.py b/test.py\n+if index < len(array):"
        mock_enhanced_result = self._create_mock_enhanced_check_result()
        
        # Mock GitDiffUtil
        with patch('siada.foundation.tools.get_git_diff.GitDiffUtil.get_git_diff_exclude_test_files') as mock_git_diff:
            mock_git_diff.return_value = mock_diff
            
            # Mock enhanced_fix_result_checker.check_with_trace
            self.agent.enhanced_fix_result_checker.check_with_trace = AsyncMock(return_value=mock_enhanced_result)
            
            # æ‰§è¡Œæµ‹è¯•ï¼ˆä¸ä¼ å…¥ run_resultï¼‰
            result = await self.agent.run_enhanced_checker(
                self.user_input, 
                self.context
            )
            
            # éªŒè¯ç»“æœ
            self.assertIsInstance(result, dict)
            self.assertTrue(result["is_fixed"])
            self.assertEqual(result["code_diff"], mock_diff)
            
            # éªŒè¯åŸºæœ¬å­—æ®µå­˜åœ¨
            self.assertIn("check_summary", result)
            self.assertIn("overall_score", result)
            self.assertIn("expert_assessment", result)
            
            # éªŒè¯ enhanced_fix_result_checker.check_with_trace è¢«è°ƒç”¨æ—¶ execution_trace ä¸º None
            self.agent.enhanced_fix_result_checker.check_with_trace.assert_called_once_with(
                issue_desc=self.user_input,
                fix_code=mock_diff,
                execution_trace=None
            )

    async def test_run_enhanced_checker_trace_extraction_failure(self):
        """æµ‹è¯•æ‰§è¡Œè½¨è¿¹æå–å¤±è´¥çš„æƒ…å†µ"""
        mock_diff = "diff content"
        mock_enhanced_result = self._create_mock_enhanced_check_result()
        
        # Mock GitDiffUtil
        with patch('siada.foundation.tools.get_git_diff.GitDiffUtil.get_git_diff_exclude_test_files') as mock_git_diff:
            mock_git_diff.return_value = mock_diff
            
            # Mock _extract_execution_trace_from_run_result æŠ›å‡ºå¼‚å¸¸
            with patch.object(self.agent, '_extract_execution_trace_from_run_result', side_effect=Exception("Trace extraction failed")):
                
                # Mock enhanced_fix_result_checker.check_with_trace
                self.agent.enhanced_fix_result_checker.check_with_trace = AsyncMock(return_value=mock_enhanced_result)
                
                # æ‰§è¡Œæµ‹è¯•
                result = await self.agent.run_enhanced_checker(
                    self.user_input, 
                    self.context, 
                    self.mock_run_result
                )
                
                # éªŒè¯å³ä½¿è½¨è¿¹æå–å¤±è´¥ï¼Œæ–¹æ³•ä»èƒ½æ­£å¸¸å·¥ä½œï¼ˆä¼ å…¥ Noneï¼‰
                self.assertIsInstance(result, dict)
                self.agent.enhanced_fix_result_checker.check_with_trace.assert_called_once_with(
                    issue_desc=self.user_input,
                    fix_code=mock_diff,
                    execution_trace=None
                )

    def test_extract_execution_trace_from_run_result(self):
        """æµ‹è¯• _extract_execution_trace_from_run_result æ–¹æ³•"""
        # ä½¿ç”¨å·²ç»è®¾ç½®å¥½çš„ mock_run_resultï¼Œå®ƒåŒ…å«äº† _create_mock_new_items() çš„æ•°æ®
        
        # æ‰§è¡Œè½¨è¿¹æå–
        trace = self.agent._extract_execution_trace_from_run_result(self.mock_run_result)
        
        # éªŒè¯è¿”å›çš„è½¨è¿¹å¯¹è±¡
        self.assertIsInstance(trace, ExecutionTrace)
        self.assertIsNotNone(trace.trace_id)
        self.assertTrue(trace.trace_id.startswith("trace_"))
        self.assertEqual(trace.workflow_name, "bug_fix")
        self.assertIsNotNone(trace.start_time)
        
        # éªŒè¯æ¨¡å‹è°ƒç”¨æ•°é‡ï¼ˆåº”è¯¥æœ‰1ä¸ª message_output_itemï¼‰
        self.assertEqual(len(trace.model_calls), 1)
        
        # éªŒè¯å·¥å…·è°ƒç”¨æ•°é‡ï¼ˆåº”è¯¥æœ‰1ä¸ª tool_call_output_itemï¼‰
        self.assertEqual(len(trace.tool_calls), 1)
        
        # éªŒè¯æ¨¡å‹è°ƒç”¨å†…å®¹
        model_call = trace.model_calls[0]
        self.assertEqual(model_call.call_id, 1)
        self.assertIn("æˆ‘åˆ†æäº†ä»£ç é—®é¢˜ï¼Œå‘ç°éœ€è¦æ·»åŠ è¾¹ç•Œæ£€æŸ¥", str(model_call.output_messages))
        
        # éªŒè¯å·¥å…·è°ƒç”¨å†…å®¹
        tool_call = trace.tool_calls[0]
        self.assertEqual(tool_call.call_id, 1)
        self.assertIn("æ–‡ä»¶å·²æˆåŠŸä¿®æ”¹", tool_call.output_result)

    # async def test_run_enhanced_checker_without_run_result(self):
    #     """æµ‹è¯•ä¸å¸¦ RunResult çš„ run_enhanced_checker æ–¹æ³•"""
    #     # å‡†å¤‡æµ‹è¯•æ•°æ®
    #     mock_diff = "diff --git a/test.py b/test.py\n+if index < len(array):"
    #     mock_enhanced_result = self._create_mock_enhanced_check_result()
        
    #     # Mock GitDiffUtil
    #     with patch('siada.foundation.tools.get_git_diff.GitDiffUtil.get_git_diff_exclude_test_files') as mock_git_diff:
    #         mock_git_diff.return_value = mock_diff
            
    #         # Mock enhanced_fix_result_checker.check
    #         self.agent.enhanced_fix_result_checker.check = AsyncMock(return_value=mock_enhanced_result)
            
    #         # æ‰§è¡Œæµ‹è¯•ï¼ˆä¸ä¼ å…¥ run_resultï¼‰
    #         result = await self.agent.run_enhanced_checker(
    #             self.user_input, 
    #             self.context
    #         )
            
    #         # éªŒè¯ç»“æœ
    #         self.assertIsInstance(result, dict)
    #         self.assertTrue(result["is_fixed"])
    #         self.assertEqual(result["code_diff"], mock_diff)
            
    #         # éªŒè¯ enhanced_fix_result_checker.check è¢«è°ƒç”¨æ—¶ execution_trace ä¸º None
    #         self.agent.enhanced_fix_result_checker.check.assert_called_once_with(
    #             issue_desc=self.user_input,
    #             fix_code=mock_diff,
    #             execution_trace=None
    #         )

    # async def test_run_enhanced_checker_with_execution_stats(self):
    #     """æµ‹è¯•åŒ…å«æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯çš„å¢å¼ºæ£€æŸ¥"""
    #     # å‡†å¤‡åŒ…å«æ‰§è¡Œç»Ÿè®¡çš„æµ‹è¯•æ•°æ®
    #     mock_enhanced_result = self._create_mock_enhanced_check_result()
    #     mock_enhanced_result["execution_stats"] = {
    #         "model_calls": 3,
    #         "tool_calls": 5,
    #         "total_duration_ms": 2500.0
    #     }
        
    #     mock_diff = "diff --git a/test.py b/test.py\n+boundary check added"
        
    #     # Mock dependencies
    #     with patch('siada.foundation.tools.get_git_diff.GitDiffUtil.get_git_diff_exclude_test_files') as mock_git_diff:
    #         mock_git_diff.return_value = mock_diff
            
    #         self.agent.enhanced_fix_result_checker.check = AsyncMock(return_value=mock_enhanced_result)
            
    #         # æ‰§è¡Œæµ‹è¯•
    #         result = await self.agent.run_enhanced_checker(
    #             self.user_input, 
    #             self.context
    #         )
            
    #         # éªŒè¯æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯
    #         self.assertIn("execution_stats", result)
    #         self.assertEqual(result["execution_stats"]["model_calls"], 3)
    #         self.assertEqual(result["execution_stats"]["tool_calls"], 5)

    # async def test_run_enhanced_checker_error_handling(self):
    #     """æµ‹è¯• run_enhanced_checker çš„é”™è¯¯å¤„ç†"""
    #     # Mock GitDiffUtil æŠ›å‡ºå¼‚å¸¸
    #     with patch('siada.foundation.tools.get_git_diff.GitDiffUtil.get_git_diff_exclude_test_files') as mock_git_diff:
    #         mock_git_diff.side_effect = Exception("Git diff failed")
            
    #         # æ‰§è¡Œæµ‹è¯•
    #         with self.assertRaises(Exception) as context:
    #             await self.agent.run_enhanced_checker(
    #                 self.user_input, 
    #                 self.context
    #             )
            
    #         self.assertIn("Git diff failed", str(context.exception))

    # async def test_run_enhanced_checker_enhanced_checker_failure(self):
    #     """æµ‹è¯•å¢å¼ºæ£€æŸ¥å™¨å¤±è´¥çš„æƒ…å†µ"""
    #     mock_diff = "diff content"
        
    #     # Mock GitDiffUtil æ­£å¸¸å·¥ä½œ
    #     with patch('siada.foundation.tools.get_git_diff.GitDiffUtil.get_git_diff_exclude_test_files') as mock_git_diff:
    #         mock_git_diff.return_value = mock_diff
            
    #         # Mock enhanced_fix_result_checker.check æŠ›å‡ºå¼‚å¸¸
    #         self.agent.enhanced_fix_result_checker.check = AsyncMock(
    #             side_effect=Exception("Enhanced checker failed")
    #         )
            
    #         # æ‰§è¡Œæµ‹è¯•
    #         with self.assertRaises(Exception) as context:
    #             await self.agent.run_enhanced_checker(
    #                 self.user_input, 
    #                 self.context
    #             )
            
    #         self.assertIn("Enhanced checker failed", str(context.exception))

    # async def test_run_enhanced_checker_trace_extraction_failure(self):
    #     """æµ‹è¯•æ‰§è¡Œè½¨è¿¹æå–å¤±è´¥çš„æƒ…å†µ"""
    #     mock_diff = "diff content"
    #     mock_enhanced_result = self._create_mock_enhanced_check_result()
        
    #     # Mock GitDiffUtil
    #     with patch('siada.foundation.tools.get_git_diff.GitDiffUtil.get_git_diff_exclude_test_files') as mock_git_diff:
    #         mock_git_diff.return_value = mock_diff
            
    #         # Mock _extract_execution_trace_from_run_result æŠ›å‡ºå¼‚å¸¸
    #         self.mock_run_result._extract_execution_trace_from_run_result.side_effect = Exception("Trace extraction failed")
            
    #         # Mock enhanced_fix_result_checker.check
    #         self.agent.enhanced_fix_result_checker.check = AsyncMock(return_value=mock_enhanced_result)
            
    #         # æ‰§è¡Œæµ‹è¯•
    #         result = await self.agent.run_enhanced_checker(
    #             self.user_input, 
    #             self.context, 
    #             self.mock_run_result
    #         )
            
    #         # éªŒè¯å³ä½¿è½¨è¿¹æå–å¤±è´¥ï¼Œæ–¹æ³•ä»èƒ½æ­£å¸¸å·¥ä½œï¼ˆä¼ å…¥ Noneï¼‰
    #         self.assertIsInstance(result, dict)
    #         self.agent.enhanced_fix_result_checker.check.assert_called_once_with(
    #             issue_desc=self.user_input,
    #             fix_code=mock_diff,
    #             execution_trace=None
    #         )

    # async def test_run_enhanced_checker_comprehensive_result_structure(self):
    #     """æµ‹è¯• run_enhanced_checker è¿”å›çš„å®Œæ•´ç»“æœç»“æ„"""
    #     mock_diff = "comprehensive diff content"
    #     mock_enhanced_result = self._create_mock_enhanced_check_result()
        
    #     with patch('siada.foundation.tools.get_git_diff.GitDiffUtil.get_git_diff_exclude_test_files') as mock_git_diff:
    #         mock_git_diff.return_value = mock_diff
            
    #         self.agent.enhanced_fix_result_checker.check = AsyncMock(return_value=mock_enhanced_result)
            
    #         result = await self.agent.run_enhanced_checker(
    #             self.user_input, 
    #             self.context
    #         )
            
    #         # éªŒè¯æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨
    #         required_fields = [
    #             "is_fixed", "check_summary", "fix_analysis", "trace_analysis",
    #             "efficiency_suggestions", "strategy_suggestions", "overall_score",
    #             "expert_assessment", "execution_intelligence", "professional_recommendations",
    #             "risk_assessment", "quality_metrics", "executive_summary", "code_diff"
    #         ]
            
    #         for field in required_fields:
    #             self.assertIn(field, result, f"Missing required field: {field}")
            
    #         # éªŒè¯ä¸“å®¶è¯„ä¼°ç»“æ„
    #         expert_assessment = result["expert_assessment"]
    #         self.assertIn("confidence_level", expert_assessment)
    #         self.assertIn("technical_depth_analysis", expert_assessment)
    #         self.assertIn("cognitive_analysis", expert_assessment)
            
    #         # éªŒè¯æ‰§è¡Œæ™ºèƒ½åˆ†æç»“æ„
    #         execution_intelligence = result["execution_intelligence"]
    #         self.assertIn("strategy_effectiveness", execution_intelligence)
    #         self.assertIn("efficiency_analysis", execution_intelligence)
    #         self.assertIn("learning_patterns", execution_intelligence)
            
    #         # éªŒè¯ä¸“ä¸šå»ºè®®ç»“æ„
    #         professional_recommendations = result["professional_recommendations"]
    #         self.assertIn("immediate_actions", professional_recommendations)
    #         self.assertIn("strategic_improvements", professional_recommendations)
    #         self.assertIn("learning_opportunities", professional_recommendations)
            
    #         # éªŒè¯é£é™©è¯„ä¼°ç»“æ„
    #         risk_assessment = result["risk_assessment"]
    #         self.assertIn("production_risks", risk_assessment)
    #         self.assertIn("technical_debt_impact", risk_assessment)
    #         self.assertIn("regression_potential", risk_assessment)
            
    #         # éªŒè¯è´¨é‡æŒ‡æ ‡ç»“æ„
    #         quality_metrics = result["quality_metrics"]
    #         self.assertIn("detailed_scores", quality_metrics)
    #         self.assertIn("score_justification", quality_metrics)
            
    #         # éªŒè¯æ‰§è¡Œæ‘˜è¦ç»“æ„
    #         executive_summary = result["executive_summary"]
    #         self.assertIn("verdict", executive_summary)
    #         self.assertIn("key_concerns", executive_summary)
    #         self.assertIn("success_criteria", executive_summary)
    #         self.assertIn("next_steps", executive_summary)

    # async def test_run_enhanced_checker_different_user_inputs(self):
    #     """æµ‹è¯•ä¸åŒç±»å‹çš„ç”¨æˆ·è¾“å…¥"""
    #     test_cases = [
    #         "ä¿®å¤å†…å­˜æ³„æ¼é—®é¢˜",
    #         "è§£å†³å¹¶å‘è®¿é—®å†²çª",
    #         "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½",
    #         "ä¿®å¤å®‰å…¨æ¼æ´",
    #         "é‡æ„ä»£ç ç»“æ„"
    #     ]
        
    #     mock_diff = "test diff"
    #     mock_enhanced_result = self._create_mock_enhanced_check_result()
        
    #     with patch('siada.foundation.tools.get_git_diff.GitDiffUtil.get_git_diff_exclude_test_files') as mock_git_diff:
    #         mock_git_diff.return_value = mock_diff
            
    #         self.agent.enhanced_fix_result_checker.check = AsyncMock(return_value=mock_enhanced_result)
            
    #         for user_input in test_cases:
    #             with self.subTest(user_input=user_input):
    #                 result = await self.agent.run_enhanced_checker(
    #                     user_input, 
    #                     self.context
    #                 )
                    
    #                 # éªŒè¯åŸºæœ¬ç»“æ„
    #                 self.assertIsInstance(result, dict)
    #                 self.assertIn("is_fixed", result)
    #                 self.assertIn("check_summary", result)
    #                 self.assertEqual(result["code_diff"], mock_diff)

    # def test_run_enhanced_checker_docstring_accuracy(self):
    #     """æµ‹è¯•æ–¹æ³•æ–‡æ¡£å­—ç¬¦ä¸²çš„å‡†ç¡®æ€§"""
    #     # è·å–æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²
    #     docstring = self.agent.run_enhanced_checker.__doc__
        
    #     # éªŒè¯æ–‡æ¡£å­—ç¬¦ä¸²åŒ…å«å…³é”®ä¿¡æ¯
    #     self.assertIsNotNone(docstring)
    #     self.assertIn("è¿è¡Œå¢å¼ºç‰ˆæ£€æŸ¥å™¨", docstring)
    #     self.assertIn("user_input", docstring)
    #     self.assertIn("context", docstring)
    #     self.assertIn("run_result", docstring)
    #     self.assertIn("Returns", docstring)
    #     self.assertIn("å¢å¼ºçš„æ£€æŸ¥ç»“æœ", docstring)


if __name__ == '__main__':
    unittest.main()
